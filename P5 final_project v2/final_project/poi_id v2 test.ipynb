{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of features (with new ones): 24\n",
      "POI:\n",
      "HANNON KEVIN P 0.65625 0.0306220095694 6.16540549872\n",
      "COLWELL WESLEY 0.275 0.136518771331 4.15883996091\n",
      "RIEKER PAULA H 0.585365853659 0.0263554216867 2.80897749206\n",
      "KOPPER MICHAEL J 0 0 3.56657230111\n",
      "SHELBY REX 0.358974358974 0.0577777777778 0.944090934839\n",
      "DELAINEY DAVID W 0.19843597263 0.0213385063046 8.21550923834\n",
      "LAY KENNETH L 0.444444444444 0.0287853966768 6.52789603113\n",
      "BOWEN JR RAYMOND M 0.555555555556 0.0753498385361 4.84563946289\n",
      "BELDEN TIMOTHY N 0.223140495868 0.0285320986109 24.5328202468\n",
      "FASTOW ANDREW S 0 0 2.94986589456\n",
      "CALGER CHRISTOPHER F 0.173611111111 0.076597382602 5.20423499827\n",
      "RICE KENNETH D 0.222222222222 0.046408839779 4.16036668283\n",
      "SKILLING JEFFREY K 0.277777777778 0.0242624758754 5.03933380007\n",
      "YEAGER F SCOTT 0 0 0\n",
      "HIRKO JOSEPH 0 0 0\n",
      "KOENIG MARK E 0.245901639344 0.0223251895535 2.25845792493\n",
      "CAUSEY RICHARD A 0.244897959184 0.0306553911205 2.40854165212\n",
      "GLISAN JR BEN F 0.375 0.0595647193585 2.18201654696\n",
      "Non-POI\n",
      "METTS MARK 0.0344827586207 0.0470879801735 1.64029437816\n",
      "BAXTER JOHN C 0 0 4.49266572321\n",
      "ELLIOTT STEVEN 0 0 2.04749006967\n",
      "CORDES WILLIAM R 0 0.0130890052356 0\n",
      "MORDAUNT KRISTINA M 0 0 1.21680463359\n",
      "MEYER ROCKFORD G 0 0 0\n",
      "MCMAHON JEFFREY 0.541666666667 0.0246284501062 7.01852891634\n",
      "HORTON STANLEY C 0.0139794967381 0.0187234042553 0\n",
      "PIPER GREGORY F 0.216216216216 0.0492730210016 2.02951935908\n",
      "HUMPHREY GENE E 1.0 0.078125 0\n",
      "UMANOFF ADAM S 0 0.108108108108 2.73312565621\n",
      "BLACHMAN JEREMY M 0.142857142857 0.010101010101 3.41989008071\n",
      "SUNDE MARTIN 0.342105263158 0.013978088402 2.71859440902\n",
      "GIBBS DANA R 0 0 0\n",
      "LOWRY CHARLES P 0 0 0\n",
      "MULLER MARK S 0 0.0882352941176 4.37108092858\n",
      "JACKSON CHARLENE R 0.339285714286 0.0968992248062 0.866376950215\n",
      "WESTFAHL RICHARD K 0 0 0\n",
      "WALTERS GARETH W 0 0 0\n",
      "WALLS JR ROBERT H 0 0.0253353204173 2.38034562618\n",
      "KITCHEN LOUISE 0.112268518519 0.0302227573751 11.420487618\n",
      "CHAN RONNIE 0 0 0\n",
      "BELFER ROBERT 0 0 0\n",
      "SHANKMAN JEFFREY A 0.0309585975382 0.0291834833903 6.57656768932\n",
      "WODRASKA JOHN 0 0 0\n",
      "BERGSIEKER RICHARD P 0 0.0104438642298 1.33033918328\n",
      "URQUHART JOHN A 0 0 0\n",
      "BIBI PHILIPPE A 0.2 0.014312383323 4.68110005851\n",
      "WHALEY DAVID A 0 0 0\n",
      "BECK SALLY W 0.0888786553074 0.0196855775803 3.02598020144\n",
      "HAUG DAVID L 0.368421052632 0.00698080279232 0\n",
      "ECHOLS JOHN B 0 0 1.09742379764\n",
      "MENDELSOHN JOHN 0 0 0\n",
      "HICKERSON GARY J 0.037037037037 0.030303030303 8.02689481935\n",
      "CLINE KENNETH W 0 0 0\n",
      "LEWIS RICHARD 0 0.0105042016807 0\n",
      "HAYES ROBERT E 0 0.031746031746 0\n",
      "MCCARTY DANNY J 0.0093023255814 0.0174459176553 0\n",
      "LEFF DANIEL P 0.222222222222 0.0237420269313 3.65302141401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\DAND\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\DAND\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\DAND\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAVORATO JOHN J 0.158994197292 0.072737291638 23.5787885218\n",
      "BERBERIAN DAVID 0 0 0\n",
      "DETMERING TIMOTHY J 0 0 2.0190023753\n",
      "WAKEHAM JOHN 0 0 0\n",
      "POWERS WILLIAM 0 0 0\n",
      "GOLD JOSEPH 0 0 2.74846086192\n",
      "BANNANTINE JAMES M 0 0.0689045936396 0\n",
      "DUNCAN JOHN H 0 0 0\n",
      "SHAPIRO RICHARD S 0.0534979423868 0.00488481087861 2.41567438196\n",
      "SHERRIFF JOHN R 0.25 0.00878569187324 3.49829749522\n",
      "LEMAISTRE CHARLES 0 0 0\n",
      "DEFFNER JOSEPH M 0.0540540540541 0.16106442577 2.91091155195\n",
      "KISHKILL JOSEPH G 0 0 0\n",
      "WHALLEY LAWRENCE G 0.0431654676259 0.0309021432132 5.8781575503\n",
      "MCCONNELL MICHAEL S 0.0707512764406 0.0276359267047 3.01338490787\n",
      "PIRO JIM 0.0625 0 0\n",
      "SULLIVAN-SHAKLOVITZ COLLEEN 0 0 0.614329858274\n",
      "WROBEL BRUCE 0 0 0\n",
      "LINDHOLM TOD A 0 0 0.845819747354\n",
      "MEYER JEROME J 0 0 0\n",
      "BUTTS ROBERT H 0 0 2.86789336025\n",
      "OLSON CINDY K 0.288461538462 0.0168918918919 2.27909492582\n",
      "MCDONALD REBECCA 0.0769230769231 0.0604026845638 0\n",
      "CUMBERLAND MICHAEL S 0 0 1.75771637489\n",
      "GAHN ROBERT S 0 0 2.65546227241\n",
      "MCCLELLAN GEORGE 0 0.0298165137615 3.41668786279\n",
      "HERMANN ROBERT J 0 0 2.66501182123\n",
      "SCRIMSHAW MATTHEW 0 0 0\n",
      "GATHMANN WILLIAM D 0 0 0\n",
      "HAEDICKE MARK E 0.0314270994333 0.0448989773011 3.07383895757\n",
      "GILLIS JOHN 0 0 0\n",
      "FITZGERALD JAY L 0.5 0.00106837606838 1.7574074725\n",
      "MORAN MICHAEL P 0 0 0\n",
      "REDMOND BRIAN L 0.221719457014 0.122082585278 0\n",
      "BAZELIDES PHILIP J 0 0 0\n",
      "DURAN WILLIAM D 0.25 0.117256637168 3.55969851727\n",
      "THORN TERENCE H 0 0 0\n",
      "FOY JOE 0 0 0\n",
      "KAMINSKI WINCENTY J 0.0119014476615 0.00889950075971 1.4540114358\n",
      "COX DAVID 0.121212121212 0 2.54543603319\n",
      "OVERDYKE JR JERE C 0 0 0\n",
      "PEREIRA PAULO V. FERRAZ 0 0 0\n",
      "STABLER FRANK 0 0 2.0876652387\n",
      "BLAKE JR. NORMAN P 0 0 0\n",
      "SHERRICK JEFFREY B 0.72 0.0636215334421 0\n",
      "PRENTICE JAMES 0 0 0\n",
      "GRAY RODNEY 0 0 0\n",
      "PICKERING MARK R 0 0.00779510022272 0.457989396019\n",
      "NOLES JAMES L 0 0 0\n",
      "KEAN STEVEN J 0.0572569906791 0.0109769484083 2.47317838046\n",
      "FOWLER PEGGY 0 0 0\n",
      "WASAFF GEORGE 0.233333333333 0.055 1.25001923107\n",
      "WHITE JR THOMAS E 0 0 1.41713090825\n",
      "CHRISTODOULOU DIOMEDES 0 0 0\n",
      "ALLEN PHILLIP K 0.0296127562642 0.0161957270848 20.6729221856\n",
      "SHARP VICTORIA T 0.0441176470588 0.00765306122449 2.41793137911\n",
      "JAEDICKE ROBERT 0 0 0\n",
      "WINOKUR JR. HERBERT S 0 0 0\n",
      "BROWN MICHAEL 0.0243902439024 0.00874831763122 0\n",
      "BADUM JAMES P 0 0 0\n",
      "HUGHES JAMES A 0.147058823529 0.0486787204451 0\n",
      "REYNOLDS LAWRENCE 0 0 1.30891765599\n",
      "DIMICHELE RICHARD G 0 0 3.80534879827\n",
      "BHATNAGAR SANJAY 0.0344827586207 0 0\n",
      "CARTER REBECCA C 0.466666666667 0.0929487179487 1.14587351848\n",
      "BUCHANAN HAROLD G 0 0 2.01599083934\n",
      "YEAP SOON 0 0 0\n",
      "MURRAY JULIA H 0.0444444444444 0.00501824817518 1.74456133005\n",
      "GARLAND C KEVIN 0.613636363636 0.0478468899522 3.66464608142\n",
      "DODSON KEITH 0.214285714286 0.0568181818182 0.31673778184\n",
      "DIETRICH JANET R 0.222222222222 0.118584758942 2.39904038385\n",
      "DERRICK JR. JAMES V 0.02200220022 0.0293443374599 1.6247778624\n",
      "FREVERT MARK A 0.285714285714 0.073893129771 1.88513495681\n",
      "PAI LOU L 0 0 3.81855742538\n",
      "BAY FRANKLIN R 0 0 1.66895452516\n",
      "HAYSLETT RODERICK J 0.0358152686145 0.0132125330313 0\n",
      "FUGH JOHN L 0 0 0\n",
      "FALLON JAMES B 0.493333333333 0.0239316239316 8.20780858077\n",
      "SAVAGE FRANK 0 0 0\n",
      "IZZO LAWRENCE L 0.263157894737 0.0564516129032 0\n",
      "TILNEY ELIZABETH A 0.578947368421 0.0217391304348 1.2129151202\n",
      "MARTIN AMANDA K 0 0.00525624178712 0\n",
      "BUY RICHARD B 0.0674264007597 0.0442804428044 2.72276778421\n",
      "GRAMM WENDY L 0 0 0\n",
      "TAYLOR MITCHELL S 0 0 2.26232401004\n",
      "DONAHUE JR JEFFREY M 0.5 0.217341040462 2.87149005208\n",
      "exercised_stock_options = 24.82\n",
      "total_stock_value = 24.18\n",
      "bonus = 20.79\n",
      "salary = 18.29\n",
      "to_poi_ratio = 16.41\n",
      "deferred_income = 11.46\n",
      "bonus_to_salary_ratio = 10.78\n",
      "long_term_incentive = 9.92\n",
      "restricted_stock = 9.21\n",
      "total_payments = 8.77\n",
      "shared_receipt_with_poi = 8.59\n",
      "loan_advances = 7.18\n",
      "expenses = 6.09\n",
      "from_poi_ratio = 3.13\n",
      "director_fees = 2.13\n",
      "to_messages = 1.65\n",
      "deferral_payments = 0.22\n",
      "from_messages = 0.17\n",
      "restricted_stock_deferred = 0.07\n",
      "['poi', 'bonus', 'exercised_stock_options', 'salary', 'total_stock_value', 'to_poi_ratio']\n",
      "GaussianNB(priors=None)\n",
      "accuracy = 0.8\n",
      "precision = 1.0\n",
      "recall = 0.2\n",
      "F1 = 0.33\n",
      "\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=40, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "accuracy = 0.68\n",
      "precision = 0.0\n",
      "recall = 0.0\n",
      "F1 = 0.0\n",
      "\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=60, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "accuracy = 0.68\n",
      "precision = 0.0\n",
      "recall = 0.0\n",
      "F1 = 0.0\n",
      "\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=100, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "accuracy = 0.75\n",
      "precision = 0.0\n",
      "recall = 0.0\n",
      "F1 = 0.0\n",
      "\n",
      "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
      "    n_clusters=2, n_init=10, n_jobs=1, precompute_distances='auto',\n",
      "    random_state=0, tol=0.0001, verbose=0)\n",
      "accuracy = 0.78\n",
      "precision = 1.0\n",
      "recall = 0.1\n",
      "F1 = 0.18\n",
      "\n",
      "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
      "    n_clusters=2, n_init=10, n_jobs=1, precompute_distances='auto',\n",
      "    random_state=None, tol=0.001, verbose=0)\n",
      "accuracy = 0.78\n",
      "precision = 1.0\n",
      "recall = 0.1\n",
      "F1 = 0.18\n",
      "\n",
      "SVC(C=10000.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "accuracy = 0.75\n",
      "precision = 0.5\n",
      "recall = 0.1\n",
      "F1 = 0.17\n",
      "\n",
      "Pipeline(steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('classifier', LogisticRegression(C=1e-08, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=42, solver='liblinear', tol=0.001,\n",
      "          verbose=0, warm_start=False))])\n",
      "accuracy = 0.75\n",
      "precision = 0.0\n",
      "recall = 0.0\n",
      "F1 = 0.0\n",
      "\n",
      "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "accuracy = 0.72\n",
      "precision = 0.0\n",
      "recall = 0.0\n",
      "F1 = 0.0\n",
      "\n",
      "Pipeline(steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('classifier', LogisticRegression(C=1e-08, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=42, solver='liblinear', tol=0.001,\n",
      "          verbose=0, warm_start=False))])\n",
      "accuracy = 0.78\n",
      "precision = 0.67\n",
      "recall = 0.2\n",
      "F1 = 0.31\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=5, max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=42,\n",
      "            verbose=0, warm_start=False)\n",
      "accuracy = 0.78\n",
      "precision = 1.0\n",
      "recall = 0.1\n",
      "F1 = 0.18\n",
      "\n",
      "GridSearchCV(cv=None, error_score='raise',\n",
      "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'),\n",
      "       fit_params={}, iid=True, n_jobs=1,\n",
      "       param_grid={'min_samples_split': [40, 60, 100]},\n",
      "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)\n",
      "accuracy = 0.68\n",
      "precision = 0.0\n",
      "recall = 0.0\n",
      "F1 = 0.0\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='distance')\n",
      "accuracy = 0.75\n",
      "precision = 0.0\n",
      "recall = 0.0\n",
      "F1 = 0.0\n",
      "\n",
      "GridSearchCV(cv=None, error_score='raise',\n",
      "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform'),\n",
      "       fit_params={}, iid=True, n_jobs=1,\n",
      "       param_grid={'n_neighbors': [3, 5, 10], 'weights': ['uniform', 'distance'], 'leaf_size': [20, 30, 50]},\n",
      "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)\n",
      "accuracy = 0.72\n",
      "precision = 0.0\n",
      "recall = 0.0\n",
      "F1 = 0.0\n",
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.85629\tPrecision: 0.49545\tRecall: 0.32650\tF1: 0.39361\tF2: 0.35040\n",
      "\tTotal predictions: 14000\tTrue positives:  653\tFalse positives:  665\tFalse negatives: 1347\tTrue negatives: 11335\n",
      "\n",
      "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
      "    n_clusters=2, n_init=10, n_jobs=1, precompute_distances='auto',\n",
      "    random_state=0, tol=0.0001, verbose=0)\n",
      "\tAccuracy: 0.78507\tPrecision: 0.26719\tRecall: 0.28950\tF1: 0.27790\tF2: 0.28474\n",
      "\tTotal predictions: 14000\tTrue positives:  579\tFalse positives: 1588\tFalse negatives: 1421\tTrue negatives: 10412\n",
      "\n",
      "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
      "    n_clusters=2, n_init=10, n_jobs=1, precompute_distances='auto',\n",
      "    random_state=None, tol=0.001, verbose=0)\n",
      "\tAccuracy: 0.77879\tPrecision: 0.25975\tRecall: 0.29650\tF1: 0.27691\tF2: 0.28834\n",
      "\tTotal predictions: 14000\tTrue positives:  593\tFalse positives: 1690\tFalse negatives: 1407\tTrue negatives: 10310\n",
      "\n",
      "Got a divide by zero when trying out: SVC(C=10000.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Precision or recall may be undefined due to a lack of true positive predicitons.\n",
      "Pipeline(steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('classifier', LogisticRegression(C=1e-08, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=42, solver='liblinear', tol=0.001,\n",
      "          verbose=0, warm_start=False))])\n",
      "\tAccuracy: 0.84150\tPrecision: 0.42422\tRecall: 0.30650\tF1: 0.35588\tF2: 0.32451\n",
      "\tTotal predictions: 14000\tTrue positives:  613\tFalse positives:  832\tFalse negatives: 1387\tTrue negatives: 11168\n",
      "\n",
      "Got a divide by zero when trying out: Pipeline(steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('classifier', LogisticRegression(C=1e-08, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=42, solver='liblinear', tol=0.001,\n",
      "          verbose=0, warm_start=False))])\n",
      "Precision or recall may be undefined due to a lack of true positive predicitons.\n",
      "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "\tAccuracy: 0.57821\tPrecision: 0.05043\tRecall: 0.10950\tF1: 0.06905\tF2: 0.08871\n",
      "\tTotal predictions: 14000\tTrue positives:  219\tFalse positives: 4124\tFalse negatives: 1781\tTrue negatives: 7876\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=5, max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=42,\n",
      "            verbose=0, warm_start=False)\n",
      "\tAccuracy: 0.84793\tPrecision: 0.43732\tRecall: 0.22500\tF1: 0.29713\tF2: 0.24920\n",
      "\tTotal predictions: 14000\tTrue positives:  450\tFalse positives:  579\tFalse negatives: 1550\tTrue negatives: 11421\n",
      "\n",
      "GridSearchCV(cv=None, error_score='raise',\n",
      "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'),\n",
      "       fit_params={}, iid=True, n_jobs=1,\n",
      "       param_grid={'min_samples_split': [40, 60, 100]},\n",
      "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)\n",
      "\tAccuracy: 0.84000\tPrecision: 0.18586\tRecall: 0.03550\tF1: 0.05961\tF2: 0.04235\n",
      "\tTotal predictions: 14000\tTrue positives:   71\tFalse positives:  311\tFalse negatives: 1929\tTrue negatives: 11689\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='distance')\n",
      "\tAccuracy: 0.88600\tPrecision: 0.67845\tRecall: 0.38400\tF1: 0.49042\tF2: 0.42050\n",
      "\tTotal predictions: 14000\tTrue positives:  768\tFalse positives:  364\tFalse negatives: 1232\tTrue negatives: 11636\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'min_samples_split': 100}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %load poi_id.py\n",
    "\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "# %load poi_id.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "#features_list = ['poi','salary'] # You will need to use more features\n",
    "features_list1 = ['poi',\n",
    "'salary',\n",
    " 'exercised_stock_options',\n",
    " 'bonus',\n",
    " 'restricted_stock_deferred',\n",
    " 'to_poi_ratio' ,\n",
    " 'from_poi_ratio' , \n",
    " 'bonus_to_salary_ratio']  \n",
    "\n",
    "features_list =[ 'poi',\n",
    " 'salary',\n",
    " 'to_messages',\n",
    " 'deferral_payments',\n",
    " 'total_payments',\n",
    " 'exercised_stock_options',\n",
    " 'bonus',\n",
    " 'restricted_stock',\n",
    " 'shared_receipt_with_poi',\n",
    " 'restricted_stock_deferred',\n",
    " 'total_stock_value',\n",
    " 'expenses',\n",
    " 'loan_advances',\n",
    " 'from_messages',\n",
    " 'director_fees',\n",
    " 'deferred_income',\n",
    " 'long_term_incentive',\n",
    " 'to_poi_ratio' ,\n",
    " 'from_poi_ratio' , \n",
    " 'bonus_to_salary_ratio']  \n",
    "\n",
    "selected_features_list = ['poi',\n",
    " 'salary',\n",
    " 'total_payments',\n",
    " 'bonus',\n",
    " 'bonus_to_salary_ratio',\n",
    " 'total_stock_value',\n",
    " 'exercised_stock_options',\n",
    " 'to_poi_ratio',\n",
    " 'deferred_income',\n",
    " 'restricted_stock',\n",
    " 'long_term_incentive']\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "### Task 2: Remove outliers\n",
    "data_dict.pop('TOTAL', 0 ) #spreadsheet total raw\n",
    "data_dict.pop('THE TRAVEL AGENCY IN THE PARK', 0) #not a person\n",
    "data_dict.pop('LOCKHART EUGENE E', 0) #contains NaNs for all features\n",
    "\n",
    "len(data_dict) #143 persons\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "### Task 3: Create new feature(s)\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "#1st and 2nd features inspired from lesson's Quiz: Feature Selection\n",
    "#3rd feature from searching about the Enron Fraud case\n",
    "\n",
    "# 1st new feature: to_poi_ratio\n",
    "for key, feature in my_dataset.iteritems():\n",
    "    if feature['from_this_person_to_poi'] == \"NaN\" or feature['from_messages'] == \"NaN\" or feature['from_this_person_to_poi']== 0:\n",
    "        feature['to_poi_ratio'] = 0\n",
    "    else:\n",
    "        feature['to_poi_ratio'] = float(feature['from_this_person_to_poi']) / float(feature['from_messages'])\n",
    "\n",
    "# 2nd new feature: from_poi_ratio\n",
    "for key, feature in my_dataset.iteritems():\n",
    "    if feature['from_poi_to_this_person'] == \"NaN\" or feature['to_messages'] == \"NaN\" or feature['from_poi_to_this_person'] == 0:\n",
    "        feature['from_poi_ratio'] = 0\n",
    "    else:\n",
    "        feature['from_poi_ratio'] = float(feature['from_poi_to_this_person']) / float(feature['to_messages'])\n",
    "\n",
    "# 3rd new feature: bonus_to_salary_ratio\n",
    "for key, feature in my_dataset.iteritems():\n",
    "    if feature['bonus'] == \"NaN\" or feature['salary'] == \"NaN\":\n",
    "        feature['bonus_to_salary_ratio'] = 0\n",
    "    else:\n",
    "        feature['bonus_to_salary_ratio'] = float(feature['bonus']) / float(feature['salary'])\n",
    "\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "\n",
    "print '# of features (with new ones):' , len(my_dataset['METTS MARK'])\n",
    "\n",
    "print 'POI:'\n",
    "# decide to keep or remove new feature by check their value with POI\n",
    "for key, feature in my_dataset.iteritems():\n",
    "    if feature['poi']:\n",
    "        print key, feature['to_poi_ratio'] , feature['from_poi_ratio'] , feature['bonus_to_salary_ratio']\n",
    "\n",
    "\n",
    "\n",
    "print 'Non-POI'\n",
    "# decide to keep or remove new feature by check their value with POI\n",
    "for key, feature in my_dataset.iteritems():\n",
    "    if not(feature['poi']):\n",
    "        print key, feature['to_poi_ratio'] , feature['from_poi_ratio'] , feature['bonus_to_salary_ratio']\n",
    "    \n",
    "        \n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list,sort_keys=True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "\n",
    "#Select K-Best features\n",
    "k = 5\n",
    "k_best = SelectKBest(k=k)\n",
    "k_best.fit(features, labels)\n",
    "scores = k_best.scores_\n",
    "unsorted_features = zip(features_list[1:], scores)\n",
    "sorted_features = list(reversed(sorted(unsorted_features, key=lambda x: x[1])))\n",
    "k_best_features = dict(sorted_features[:k])\n",
    "\n",
    "\n",
    "\n",
    "selected_features_list = ['poi'] + k_best_features.keys()\n",
    "\n",
    "\n",
    "for key, value in sorted_features:\n",
    "    print key , '=' , round(value,2)\n",
    "\n",
    "print selected_features_list\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, selected_features_list)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "#Scale features using MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "features_train, features_test, labels_train, labels_test =     train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "    \n",
    "# Provided to give you a starting point. Try a variety of classifiers.\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import tree\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm, grid_search\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "\n",
    "def test_clf(c):\n",
    "    clf = c\n",
    "    clf.fit(features_train, labels_train)\n",
    "    predictions = clf.predict(features_test)\n",
    "    accuracy = accuracy_score(labels_test, predictions)\n",
    "    precision = precision_score(labels_test, predictions)\n",
    "    recall = recall_score(labels_test, predictions)\n",
    "    F1 = f1_score(labels_test, predictions)\n",
    "    \n",
    "    #precision =  precision_score(features_test,labels_test,average='weighted')\n",
    "    #recall =recall_score(features_test,labels_test,average='weighted')\n",
    "    #F1 = f1_score(features_test,labels_test,average='weighted')\n",
    "\n",
    "    print c\n",
    "    print 'accuracy =' , round(accuracy ,2)\n",
    "    print 'precision =', round(precision ,2)\n",
    "    print 'recall =', round(recall ,2)\n",
    "    print 'F1 =', round(F1 ,2)\n",
    "    \n",
    "clf = GaussianNB()\n",
    "test_clf(GaussianNB())\n",
    "print''\n",
    "test_clf(tree.DecisionTreeClassifier(min_samples_split=40))\n",
    "print''\n",
    "test_clf(tree.DecisionTreeClassifier(min_samples_split=60))\n",
    "print''\n",
    "test_clf(tree.DecisionTreeClassifier(min_samples_split=100))\n",
    "print''\n",
    "test_clf(KMeans(n_clusters=2, random_state=0))\n",
    "print''\n",
    "test_clf(KMeans(n_clusters=2, tol=0.001))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "# Example starting point. Try investigating other evaluation techniques!\n",
    "\n",
    "# using Pipline and GridSearch for tuning classifiers\n",
    "\n",
    "print''\n",
    "test_clf(SVC(kernel=\"rbf\", C=10000.0))\n",
    "lclf = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', LogisticRegression(tol = 0.001, C = 10**-8, penalty = 'l2', random_state = 42))])\n",
    "\n",
    "\n",
    "lclf2 = Pipeline(steps=[\n",
    "        ('scaler', preprocessing.MinMaxScaler()),\n",
    "        ('classifier', LogisticRegression(tol = 0.001, C = 10**-8, penalty = 'l2', random_state = 42))])\n",
    "print''\n",
    "test_clf(lclf2)\n",
    "\n",
    "print''\n",
    "test_clf(LogisticRegression(C=1e5))\n",
    "\n",
    "print''\n",
    "test_clf(lclf)\n",
    "clf = rfclf = RandomForestClassifier(max_depth = 5,max_features = 'sqrt',n_estimators = 10, random_state = 42)\n",
    "print''\n",
    "test_clf(rfclf)\n",
    "\n",
    "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10,10000.0]}\n",
    "svr = SVC()\n",
    "svrclf = grid_search.GridSearchCV(svr, parameters) #too long in tester\n",
    "\n",
    "#print''\n",
    "#test_clf(svrclf)\n",
    "\n",
    "DT = tree.DecisionTreeClassifier()\n",
    "parameters = {'min_samples_split':[40,60, 100]}\n",
    "DTclf = grid_search.GridSearchCV(DT, parameters)\n",
    "print''\n",
    "test_clf(DTclf)\n",
    "\n",
    "print''\n",
    "test_clf(KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "           metric_params=None, n_neighbors=5, p=2, weights='distance'))\n",
    "\n",
    "KNeighbors = KNeighborsClassifier()\n",
    "parameters = {'leaf_size':[20,30,50], 'n_neighbors':[3,5, 10], 'weights':['uniform', 'distance']}\n",
    "KNclf = grid_search.GridSearchCV(KNeighbors, parameters)\n",
    "\n",
    "print''\n",
    "test_clf(KNclf)\n",
    "\n",
    "\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "clf1 = GaussianNB()\n",
    "clf2= tree.DecisionTreeClassifier(min_samples_split=40)\n",
    "clf3 = tree.DecisionTreeClassifier(min_samples_split=60)\n",
    "clf4 = tree.DecisionTreeClassifier(min_samples_split=100)\n",
    "clf5 = KMeans(n_clusters=2, random_state=0)\n",
    "clf6 = KMeans(n_clusters=2, tol=0.001)\n",
    "clf7 = SVC(kernel=\"rbf\", C=10000.0)\n",
    "clf8 = lclf\n",
    "clf9 = lclf2\n",
    "clf10 = LogisticRegression(C=1e5)\n",
    "clf11 = rfclf\n",
    "#clf12 = svrclf\n",
    "clf13 = DTclf\n",
    "clf14 = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "           metric_params=None, n_neighbors=5, p=2, weights='distance')\n",
    "clf15 = KNclf\n",
    "\n",
    "\n",
    "#select best classifier (KNeighborsClassifier)\n",
    "\n",
    "clf=clf14\n",
    "\n",
    "\n",
    "# In[24]:\n",
    "\n",
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, selected_features_list)\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "# %load tester.py\n",
    "#!/usr/bin/pickle\n",
    "\n",
    "\"\"\" a basic script for importing student's POI identifier,\n",
    "    and checking the results that they get from it \n",
    " \n",
    "    requires that the algorithm, dataset, and features list\n",
    "    be written to my_classifier.pkl, my_dataset.pkl, and\n",
    "    my_feature_list.pkl, respectively\n",
    "\n",
    "    that process should happen at the end of poi_id.py\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "\n",
    "PERF_FORMAT_STRING = \"\\tAccuracy: {:>0.{display_precision}f}\\tPrecision: {:>0.{display_precision}f}\\tRecall: {:>0.{display_precision}f}\\tF1: {:>0.{display_precision}f}\\tF2: {:>0.{display_precision}f}\"\n",
    "RESULTS_FORMAT_STRING = \"\\tTotal predictions: {:4d}\\tTrue positives: {:4d}\\tFalse positives: {:4d}\\tFalse negatives: {:4d}\\tTrue negatives: {:4d}\"\n",
    "\n",
    "def test_classifier(clf, dataset, feature_list, folds = 1000):\n",
    "    data = featureFormat(dataset, feature_list, sort_keys = True)\n",
    "    labels, features = targetFeatureSplit(data)\n",
    "    cv = StratifiedShuffleSplit(labels, folds, random_state = 42)\n",
    "    true_negatives = 0\n",
    "    false_negatives = 0\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    for train_idx, test_idx in cv: \n",
    "        features_train = []\n",
    "        features_test  = []\n",
    "        labels_train   = []\n",
    "        labels_test    = []\n",
    "        for ii in train_idx:\n",
    "            features_train.append( features[ii] )\n",
    "            labels_train.append( labels[ii] )\n",
    "        for jj in test_idx:\n",
    "            features_test.append( features[jj] )\n",
    "            labels_test.append( labels[jj] )\n",
    "        \n",
    "        ### fit the classifier using training set, and test on test set\n",
    "        clf.fit(features_train, labels_train)\n",
    "        predictions = clf.predict(features_test)\n",
    "        for prediction, truth in zip(predictions, labels_test):\n",
    "            if prediction == 0 and truth == 0:\n",
    "                true_negatives += 1\n",
    "            elif prediction == 0 and truth == 1:\n",
    "                false_negatives += 1\n",
    "            elif prediction == 1 and truth == 0:\n",
    "                false_positives += 1\n",
    "            elif prediction == 1 and truth == 1:\n",
    "                true_positives += 1\n",
    "            else:\n",
    "                print \"Warning: Found a predicted label not == 0 or 1.\"\n",
    "                print \"All predictions should take value 0 or 1.\"\n",
    "                print \"Evaluating performance for processed predictions:\"\n",
    "                break\n",
    "    try:\n",
    "        total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
    "        accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
    "        precision = 1.0*true_positives/(true_positives+false_positives)\n",
    "        recall = 1.0*true_positives/(true_positives+false_negatives)\n",
    "        f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
    "        f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
    "        print clf\n",
    "        print PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5)\n",
    "        print RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives)\n",
    "        print \"\"\n",
    "    except:\n",
    "        print \"Got a divide by zero when trying out:\", clf\n",
    "        print \"Precision or recall may be undefined due to a lack of true positive predicitons.\"\n",
    "\n",
    "CLF_PICKLE_FILENAME = \"my_classifier.pkl\"\n",
    "DATASET_PICKLE_FILENAME = \"my_dataset.pkl\"\n",
    "FEATURE_LIST_FILENAME = \"my_feature_list.pkl\"\n",
    "\n",
    "def dump_classifier_and_data(clf, dataset, feature_list):\n",
    "    with open(CLF_PICKLE_FILENAME, \"w\") as clf_outfile:\n",
    "        pickle.dump(clf, clf_outfile)\n",
    "    with open(DATASET_PICKLE_FILENAME, \"w\") as dataset_outfile:\n",
    "        pickle.dump(dataset, dataset_outfile)\n",
    "    with open(FEATURE_LIST_FILENAME, \"w\") as featurelist_outfile:\n",
    "        pickle.dump(feature_list, featurelist_outfile)\n",
    "\n",
    "def load_classifier_and_data():\n",
    "    with open(CLF_PICKLE_FILENAME, \"r\") as clf_infile:\n",
    "        clf = pickle.load(clf_infile)\n",
    "    with open(DATASET_PICKLE_FILENAME, \"r\") as dataset_infile:\n",
    "        dataset = pickle.load(dataset_infile)\n",
    "    with open(FEATURE_LIST_FILENAME, \"r\") as featurelist_infile:\n",
    "        feature_list = pickle.load(featurelist_infile)\n",
    "    return clf, dataset, feature_list\n",
    "\n",
    "def main():\n",
    "    ### load up student's classifier, dataset, and feature_list\n",
    "    clf, dataset, feature_list = load_classifier_and_data()\n",
    "    ### Run testing script\n",
    "  \n",
    "    test_classifier(clf1, dataset, feature_list)\n",
    "    #test_classifier(clf2, dataset, feature_list)\n",
    "    #test_classifier(clf3, dataset, feature_list)\n",
    "    #test_classifier(clf4, dataset, feature_list)\n",
    "    test_classifier(clf5, dataset, feature_list)\n",
    "    test_classifier(clf6, dataset, feature_list)\n",
    "    test_classifier(clf7, dataset, feature_list)\n",
    "    test_classifier(clf8, dataset, feature_list)\n",
    "    test_classifier(clf9, dataset, feature_list)\n",
    "    test_classifier(clf10, dataset, feature_list)\n",
    "    test_classifier(clf11, dataset, feature_list)\n",
    "    #test_classifier(clf12, dataset, feature_list)\n",
    "    test_classifier(clf13, dataset, feature_list)\n",
    "    test_classifier(clf14, dataset, feature_list)\n",
    "    #test_classifier(clf15, dataset, feature_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "\n",
    "# In[27]:\n",
    "\n",
    "\n",
    "KNclf.best_params_\n",
    "DTclf.best_params_ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='distance')\n",
      "\tAccuracy: 0.88600\tPrecision: 0.67845\tRecall: 0.38400\tF1: 0.49042\tF2: 0.42050\n",
      "\tTotal predictions: 14000\tTrue positives:  768\tFalse positives:  364\tFalse negatives: 1232\tTrue negatives: 11636\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %load tester.py\n",
    "#!/usr/bin/pickle\n",
    "\n",
    "\"\"\" a basic script for importing student's POI identifier,\n",
    "    and checking the results that they get from it \n",
    " \n",
    "    requires that the algorithm, dataset, and features list\n",
    "    be written to my_classifier.pkl, my_dataset.pkl, and\n",
    "    my_feature_list.pkl, respectively\n",
    "\n",
    "    that process should happen at the end of poi_id.py\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "\n",
    "PERF_FORMAT_STRING = \"\\\n",
    "\\tAccuracy: {:>0.{display_precision}f}\\tPrecision: {:>0.{display_precision}f}\\t\\\n",
    "Recall: {:>0.{display_precision}f}\\tF1: {:>0.{display_precision}f}\\tF2: {:>0.{display_precision}f}\"\n",
    "RESULTS_FORMAT_STRING = \"\\tTotal predictions: {:4d}\\tTrue positives: {:4d}\\tFalse positives: {:4d}\\\n",
    "\\tFalse negatives: {:4d}\\tTrue negatives: {:4d}\"\n",
    "\n",
    "def test_classifier(clf, dataset, feature_list, folds = 1000):\n",
    "    data = featureFormat(dataset, feature_list, sort_keys = True)\n",
    "    labels, features = targetFeatureSplit(data)\n",
    "    cv = StratifiedShuffleSplit(labels, folds, random_state = 42)\n",
    "    true_negatives = 0\n",
    "    false_negatives = 0\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    for train_idx, test_idx in cv: \n",
    "        features_train = []\n",
    "        features_test  = []\n",
    "        labels_train   = []\n",
    "        labels_test    = []\n",
    "        for ii in train_idx:\n",
    "            features_train.append( features[ii] )\n",
    "            labels_train.append( labels[ii] )\n",
    "        for jj in test_idx:\n",
    "            features_test.append( features[jj] )\n",
    "            labels_test.append( labels[jj] )\n",
    "        \n",
    "        ### fit the classifier using training set, and test on test set\n",
    "        clf.fit(features_train, labels_train)\n",
    "        predictions = clf.predict(features_test)\n",
    "        for prediction, truth in zip(predictions, labels_test):\n",
    "            if prediction == 0 and truth == 0:\n",
    "                true_negatives += 1\n",
    "            elif prediction == 0 and truth == 1:\n",
    "                false_negatives += 1\n",
    "            elif prediction == 1 and truth == 0:\n",
    "                false_positives += 1\n",
    "            elif prediction == 1 and truth == 1:\n",
    "                true_positives += 1\n",
    "            else:\n",
    "                print \"Warning: Found a predicted label not == 0 or 1.\"\n",
    "                print \"All predictions should take value 0 or 1.\"\n",
    "                print \"Evaluating performance for processed predictions:\"\n",
    "                break\n",
    "    try:\n",
    "        total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
    "        accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
    "        precision = 1.0*true_positives/(true_positives+false_positives)\n",
    "        recall = 1.0*true_positives/(true_positives+false_negatives)\n",
    "        f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
    "        f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
    "        print clf\n",
    "        print PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5)\n",
    "        print RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives)\n",
    "        print \"\"\n",
    "    except:\n",
    "        print \"Got a divide by zero when trying out:\", clf\n",
    "        print \"Precision or recall may be undefined due to a lack of true positive predicitons.\"\n",
    "\n",
    "CLF_PICKLE_FILENAME = \"my_classifier.pkl\"\n",
    "DATASET_PICKLE_FILENAME = \"my_dataset.pkl\"\n",
    "FEATURE_LIST_FILENAME = \"my_feature_list.pkl\"\n",
    "\n",
    "def dump_classifier_and_data(clf, dataset, feature_list):\n",
    "    with open(CLF_PICKLE_FILENAME, \"w\") as clf_outfile:\n",
    "        pickle.dump(clf, clf_outfile)\n",
    "    with open(DATASET_PICKLE_FILENAME, \"w\") as dataset_outfile:\n",
    "        pickle.dump(dataset, dataset_outfile)\n",
    "    with open(FEATURE_LIST_FILENAME, \"w\") as featurelist_outfile:\n",
    "        pickle.dump(feature_list, featurelist_outfile)\n",
    "\n",
    "def load_classifier_and_data():\n",
    "    with open(CLF_PICKLE_FILENAME, \"r\") as clf_infile:\n",
    "        clf = pickle.load(clf_infile)\n",
    "    with open(DATASET_PICKLE_FILENAME, \"r\") as dataset_infile:\n",
    "        dataset = pickle.load(dataset_infile)\n",
    "    with open(FEATURE_LIST_FILENAME, \"r\") as featurelist_infile:\n",
    "        feature_list = pickle.load(featurelist_infile)\n",
    "    return clf, dataset, feature_list\n",
    "\n",
    "def main():\n",
    "    ### load up student's classifier, dataset, and feature_list\n",
    "    clf, dataset, feature_list = load_classifier_and_data()\n",
    "    ### Run testing script\n",
    "    test_classifier(clf, dataset, feature_list)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DAND]",
   "language": "python",
   "name": "conda-env-DAND-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
